{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/data/anaconda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "from PIL import Image\n",
    "import skimage.data\n",
    "import skimage.exposure\n",
    "import skimage.color\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make training pairs parent func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makePairsParent(train_info, k):\n",
    "\n",
    "    artists = train_info.artist.unique()       \n",
    "    np.random.shuffle(artists)\n",
    "    \n",
    "    chunkSize = int(len(artists)/k)\n",
    "    \n",
    "    artist_list_list = []\n",
    "    \n",
    "    for foldNum in range(k):\n",
    "        if foldNum == k-1:\n",
    "            artist_list_list.append( artists[foldNum*chunkSize : len(artists)] )\n",
    "        else:\n",
    "            artist_list_list.append( artists[foldNum*chunkSize : (foldNum+1)*(chunkSize) ] )\n",
    "        \n",
    "    # define which indices belong to each fold\n",
    "\n",
    "    #foldLocsList = [] #list of Index objects, one for each fold\n",
    "\n",
    "    #foldSize = int(len(train_info)/k)\n",
    "\n",
    "    #for foldNum in range(k):\n",
    "    #    if foldNum == k-1:\n",
    "    #        foldLocsList.append( train_info.index[foldNum*foldSize : len(train_info)] )\n",
    "    #    else:\n",
    "    #        foldLocsList.append( train_info.index[foldNum*foldSize : (foldNum+1)*(foldSize) ] )\n",
    "\n",
    "    # define which indices belong to each fold\n",
    "    argsList = []\n",
    "\n",
    "    num_cores = min( multiprocessing.cpu_count()-2, k)\n",
    "\n",
    "    for foldNum in range(k):\n",
    "        #argsList.append([train_info.loc[foldLocsList[foldNum]]])\n",
    "        argsList.append( [ train_info[train_info.artist.isin(artist_list_list[foldNum])], foldNum ] )\n",
    "\n",
    "    print('Launching %s jobs to make pairs' % k)\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    train_pairs_list = pool.starmap(make_pairs, argsList)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    endTime = time.time()\n",
    "\n",
    "    print(\"making pairs complete, time taken = %.2f minutes\" % ((endTime - startTime) / 60.0))\n",
    "    \n",
    "    return pd.concat(train_pairs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_pairs(train_info, foldNum):\n",
    "    \"\"\"Creates training data from the supplied training image information file\"\"\"\n",
    "    artists = train_info.artist.unique()\n",
    "\n",
    "    n = train_info.groupby('artist').size()\n",
    "    n = (2*n**2).sum() \n",
    "    t = pd.DataFrame(np.zeros((n, 4)), columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "    i = 0\n",
    "    j = 0\n",
    "    \n",
    "    for artist in artists:\n",
    "\n",
    "        #artist info is Ax2 matrix of artist, filename\n",
    "        artistInfo = train_info[train_info.artist==artist][['artist', 'filename']].values\n",
    "        \n",
    "        use = train_info[train_info.artist != artist ].index.values\n",
    "        np.random.shuffle(use)\n",
    "        \n",
    "        #nm = np.min([a.shape[0]**2, train_info[train_info.artist != m].shape[0] ])\n",
    "        numExamples = np.min([len(artistInfo)**2, sum(train_info.artist != artist) ])\n",
    "        use = use[0:numExamples]\n",
    "        \n",
    "        #diffArtistInfo a Bx2 matrix of artist, filename\n",
    "        diffArtistInfo = train_info[train_info.artist!=artist][['artist', 'filename']].ix[use, :].values\n",
    "\n",
    "        \n",
    "        toAdd_SameArtist = pd.DataFrame(np.concatenate([  np.repeat(artistInfo[:, 0], len(artistInfo)).reshape((-1,1)), #artist\n",
    "                                            np.repeat(artistInfo[:, 1],\n",
    "                                            artistInfo.shape[0]).reshape((-1,1)),\n",
    "                                            np.tile(artistInfo, (len(artistInfo), 1))],\n",
    "                                         axis=1),\n",
    "                          columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "        toAdd_SameArtist = toAdd_SameArtist.loc[0:numExamples, :]\n",
    "        \n",
    "        toAdd_DiffArtist = pd.DataFrame(np.concatenate([np.tile(artistInfo,\n",
    "                                                  (len(artistInfo), 1))[0:len(diffArtistInfo), :],\n",
    "                                          diffArtistInfo], axis=1),\n",
    "                          columns=['artist1', 'image1', 'artist2', 'image2'])\n",
    "        toAdd_DiffArtist = toAdd_DiffArtist.loc[0:numExamples, :]\n",
    "        \n",
    "        #print(j, i, a2.shape[0], b2.shape[0])\n",
    "        #print(b2)\n",
    "        t.iloc[i:i+len(toAdd_SameArtist), :] = toAdd_SameArtist.values\n",
    "        t.iloc[i+len(toAdd_SameArtist):i+len(toAdd_SameArtist)+len(toAdd_DiffArtist), :] = toAdd_DiffArtist.values\n",
    "        \n",
    "        i += len(toAdd_SameArtist) + len(toAdd_DiffArtist)\n",
    "        j += 1\n",
    "        if j%100==0:\n",
    "            print('finished %s of %s artists'%(j, len(artists)))\n",
    "\n",
    "    print('make pairs completed')\n",
    "    t = t[~t.image2.isin([np.nan, 0])]\n",
    "    \n",
    "    t['sameArtist'] = ( t['artist1'] == t['artist2'] )\n",
    "    t['foldNum'] = foldNum\n",
    "    \n",
    "    return t[t.image1 > t.image2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep Image List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def prepImageList(image_info, isTest):\n",
    "#     \"\"\"given the train_image_info or submission_info, returns a dataframe with a single column containing filenames of images\"\"\"\n",
    "#     if isTest:\n",
    "#         images = list(set(list(image_info.image1.unique()) + list(image_info.image2.unique())))\n",
    "#         result = pd.DataFrame(np.array(images).reshape((-1, 1)), columns = ['filename'])\n",
    "#     else:\n",
    "#         result = pd.DataFrame(columns = ['filename'], data = image_info['filename'] )\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features Parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeaturesParent(isTest):\n",
    "    \"\"\"Creates features for training and test images. This function utilizes multiprocessing.\n",
    "    Args:\n",
    "        isTest: bool to fetch training or test data\n",
    "    Returns:\n",
    "        pandas DataFrame containing features\n",
    "    \"\"\"\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count() - 5\n",
    "    \n",
    "    argsList = []\n",
    "    \n",
    "    for jobNum in range(num_cores):\n",
    "        argsList.append((isTest, jobNum, num_cores))\n",
    "        \n",
    "\n",
    "    print('Launching %s jobs' % (num_cores))\n",
    "    startTime = time.time()\n",
    "    \n",
    "    pool = multiprocessing.Pool(num_cores)\n",
    "    image_features_list = pool.starmap(getFeaturesWorker, argsList)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    image_features = pd.concat(image_features_list)\n",
    "    \n",
    "    endTime = time.time()\n",
    "    \n",
    "    print(\"collecting features complete, time taken = %.2f minutes\" % ((endTime - startTime) / 60.0))\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getFeaturesWorker(isTest, jobNum, totalJobs):\n",
    "    \"\"\"Child function for computing image features, only to be called by getFeaturesParent\n",
    "    Args:\n",
    "        isTest: whether to compute features for test or training images\n",
    "        jobNum: which job number this is\n",
    "        totalJobs: total number of jobs\n",
    "    Returns:\n",
    "        pandas dataframe containing a data for a fraction of the training or test images\n",
    "    \"\"\"\n",
    "    if isTest:\n",
    "        mydir = r'/data/test_data/test'\n",
    "        info = pd.read_csv(r'/data/test_data/submission_info.csv', index_col = 0)\n",
    "    else:\n",
    "        mydir = r'/data/training_data/train'\n",
    "        info = pd.read_csv(r'/data/training_data/train_info.csv', index_col = 0)\n",
    "    \n",
    "    info = info[0:2000]\n",
    "    \n",
    "    totalNumImages = len(info)\n",
    "    \n",
    "    chunkSize = np.int(totalNumImages/totalJobs)\n",
    "    \n",
    "    if jobNum == totalJobs - 1:\n",
    "        startInd = jobNum * chunkSize\n",
    "        endInd = totalNumImages\n",
    "    else:\n",
    "        startInd = jobNum * chunkSize\n",
    "        endInd = (jobNum + 1) * chunkSize\n",
    "        \n",
    "    info = info.iloc[startInd:endInd]\n",
    "    \n",
    "    info['pixelsx'] = np.nan\n",
    "    info['pixelsy'] = np.nan\n",
    "    info['size_bytes'] = np.nan\n",
    "    \n",
    "    info['r_mean'] = np.nan\n",
    "    info['r_10_pct'] = np.nan\n",
    "    info['r_25_pct'] = np.nan\n",
    "    info['r_50_pct'] = np.nan\n",
    "    info['r_75_pct'] = np.nan\n",
    "    info['r_90_pct'] = np.nan\n",
    "    \n",
    "    info['g_mean'] = np.nan\n",
    "    info['g_std'] = np.nan\n",
    "    info['g_10_pct'] = np.nan\n",
    "    info['g_25_pct'] = np.nan\n",
    "    info['g_50_pct'] = np.nan\n",
    "    info['g_75_pct'] = np.nan\n",
    "    info['g_90_pct'] = np.nan\n",
    "    \n",
    "    info['b_mean'] = np.nan\n",
    "    info['b_std'] = np.nan\n",
    "    info['b_10_pct'] = np.nan\n",
    "    info['b_25_pct'] = np.nan\n",
    "    info['b_50_pct'] = np.nan\n",
    "    info['b_75_pct'] = np.nan\n",
    "    info['b_90_pct'] = np.nan\n",
    "    \n",
    "    info['h_mean'] = np.nan\n",
    "    info['h_var'] = np.nan\n",
    "    \n",
    "    info['s_mean'] = np.nan\n",
    "    info['s_std'] = np.nan\n",
    "    info['s_10_pct'] = np.nan \n",
    "    info['s_25_pct'] = np.nan \n",
    "    info['s_50_pct'] = np.nan \n",
    "    info['s_75_pct'] = np.nan \n",
    "    info['s_90_pct'] = np.nan \n",
    "    \n",
    "    info['v_mean'] = np.nan\n",
    "    info['v_std'] = np.nan\n",
    "    info['v_10_pct'] = np.nan \n",
    "    info['v_25_pct'] = np.nan \n",
    "    info['v_50_pct'] = np.nan\n",
    "    info['v_75_pct'] = np.nan \n",
    "    info['v_90_pct'] = np.nan \n",
    "    \n",
    "    info['v_10_pct'] = np.nan \n",
    "    info['v_25_pct'] = np.nan \n",
    "    info['v_50_pct'] = np.nan\n",
    "    info['v_75_pct'] = np.nan \n",
    "    info['v_90_pct'] = np.nan \n",
    "   \n",
    "    info['h_cx_05_pct'] = np.nan \n",
    "    info['h_cx_10_pct'] = np.nan \n",
    "    info['h_cx_25_pct'] = np.nan \n",
    "    info['h_cx_50_pct'] = np.nan\n",
    "    info['h_cx_75_pct'] = np.nan \n",
    "    info['h_cx_90_pct'] = np.nan \n",
    "    info['h_cx_95_pct'] = np.nan \n",
    "    \n",
    "    info['s_cx_05_pct'] = np.nan \n",
    "    info['s_cx_10_pct'] = np.nan \n",
    "    info['s_cx_25_pct'] = np.nan \n",
    "    info['s_cx_50_pct'] = np.nan\n",
    "    info['s_cx_75_pct'] = np.nan \n",
    "    info['s_cx_90_pct'] = np.nan \n",
    "    info['s_cx_95_pct'] = np.nan\n",
    "    \n",
    "    info['v_cx_05_pct'] = np.nan \n",
    "    info['v_cx_10_pct'] = np.nan \n",
    "    info['v_cx_25_pct'] = np.nan \n",
    "    info['v_cx_50_pct'] = np.nan\n",
    "    info['v_cx_75_pct'] = np.nan \n",
    "    info['v_cx_90_pct'] = np.nan \n",
    "    info['v_cx_95_pct'] = np.nan\n",
    "    \n",
    "    info['is_grayscale'] = np.nan\n",
    "      \n",
    "    print('Job %s, starting getting image info for images %s-%s' % (jobNum, startInd, endInd-1))\n",
    "    startTime = time.clock()\n",
    "    \n",
    "    for ind, i in enumerate(info.index.values):\n",
    "        try:       \n",
    "            #im = Image.open(mydir+'/'+info.loc[i, 'filename'])\n",
    "            #info.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.size\n",
    "            \n",
    "            im = skimage.data.imread(mydir + '/' + info.loc[i, 'filename'])\n",
    "                \n",
    "            info.loc[i, 'pixelsx'] = pixelsx = im.shape[1]\n",
    "            info.loc[i, 'pixelsy'] = pixelsy = im.shape[0]\n",
    "            \n",
    "            info.loc[i, 'is_grayscale' ] = grayscale = (len(im.shape) == 2)\n",
    "            \n",
    "            # get sample dimensions\n",
    "            yxRatio = (pixelsy/pixelsx)\n",
    "\n",
    "            base_size = 200\n",
    "            \n",
    "            if yxRatio < 1.0:\n",
    "                num_y_samp = base_size + 2\n",
    "                num_x_samp = np.int( num_y_samp / yxRatio )\n",
    "            else:\n",
    "                num_x_samp = base_size + 2\n",
    "                num_y_samp = np.int( num_x_samp * yxRatio)\n",
    "\n",
    "            #print('x and y: %i %i' %(pixelsx, pixelsy))\n",
    "            #sample\n",
    "            x_space = np.round(np.linspace(0, pixelsx-1, num_x_samp)).astype(int)\n",
    "            y_space = np.round(np.linspace(0, pixelsy-1, num_y_samp)).astype(int)\n",
    "\n",
    "            samp_im = np.take(im, x_space, axis=1)\n",
    "            samp_im = np.take(samp_im, y_space, axis=0)\n",
    "            \n",
    "            im = samp_im\n",
    "                \n",
    "            #convert grayscale to rgb\n",
    "            if grayscale:\n",
    "                temp = np.zeros([num_y_samp, num_x_samp, 3])\n",
    "                #temp = np.zeros([pixelsy, pixelsx, 3])\n",
    "                for ind in range(3):\n",
    "                    temp[:,:,ind] = im\n",
    "                im = temp    \n",
    "              \n",
    "            # rgb \n",
    "            info.loc[i, 'r_mean'] = im[:,:,0].mean()\n",
    "            info.loc[i, 'g_mean'] = im[:,:,1].mean()\n",
    "            info.loc[i, 'b_mean'] = im[:,:,2].mean()\n",
    "            \n",
    "            r_pcts = np.percentile(im[:,:,0],[10, 25, 50, 75, 90])\n",
    "            b_pcts = np.percentile(im[:,:,1],[10, 25, 50, 75, 90])\n",
    "            g_pcts = np.percentile(im[:,:,2],[10, 25, 50, 75, 90])\n",
    "            \n",
    "            for ind, percentile in enumerate([10, 25, 50, 75, 90]):\n",
    "                info.loc[i, 'r_%.2i_pct' % percentile] = r_pcts[ind]\n",
    "                info.loc[i, 'g_%.2i_pct' % percentile] = g_pcts[ind]\n",
    "                info.loc[i, 'b_%.2i_pct' % percentile] = b_pcts[ind]\n",
    "            \n",
    "            info.loc[i, 'r_std'] = im[:,:,0].std()\n",
    "            info.loc[i, 'g_std'] = im[:,:,1].std()\n",
    "            info.loc[i, 'b_std'] = im[:,:,2].std()\n",
    "\n",
    "            #if it is in RGBA, we don't handle it for now\n",
    "            if (len(im.shape) == 3) and im.shape[2] == 4:\n",
    "                print('%s is rgba' % info.loc[i, 'filename'])\n",
    "            else:     \n",
    "                # convert image to hue/saturation/value\n",
    "                hsvImage = skimage.color.rgb2hsv(im)\n",
    "                h_angles = hsvImage[:,:,0] * 2.0 * np.pi\n",
    "\n",
    "                # average hue is converting the (0-1) hue value to unit vector coordinates\n",
    "                # and finding the average direction\n",
    "                sinSum = np.sin(h_angles).sum()\n",
    "                cosSum = np.cos(h_angles).sum()\n",
    "                info.loc[i, 'h_mean'] = np.arctan(sinSum/cosSum)\n",
    "\n",
    "                # use the variance formula for a circulator distribution\n",
    "                R2 = np.power(sinSum, 2) + np.power(cosSum, 2)\n",
    "                numPixels = info.loc[i, 'pixelsx'] * info.loc[i, 'pixelsy']\n",
    "                R_bar = np.sqrt(R2)/numPixels\n",
    "                info.loc[i, 'h_var'] = 1 - R_bar\n",
    "\n",
    "                info.loc[i, 's_mean'] = hsvImage[:,:,1].mean()\n",
    "                info.loc[i, 's_std'] = hsvImage[:,:,1].std()\n",
    "\n",
    "                info.loc[i, 'v_mean'] = hsvImage[:,:,2].mean()\n",
    "                info.loc[i, 'v_std'] = hsvImage[:,:,2].std()\n",
    "                \n",
    "                s_pcts = np.percentile(im[:,:,1],[10, 25, 50, 75, 90])\n",
    "                v_pcts = np.percentile(im[:,:,2],[10, 25, 50, 75, 90])\n",
    "\n",
    "                for ind, percentile in enumerate([10, 25, 50, 75, 90]):\n",
    "                    info.loc[i, 's_%.2i_pct' % percentile] = r_pcts[ind]\n",
    "                    info.loc[i, 'v_%.2i_pct' % percentile] = g_pcts[ind]\n",
    "\n",
    "                #complexity metrics\n",
    "                \n",
    "                #print('samp im shape' % samp_im.shape)\n",
    "                \n",
    "                # go from hue to angles\n",
    "                hsvImageCopy = hsvImage.copy()\n",
    "                \n",
    "                samp_h_angles = hsvImage[:,:,0] * 2.0 * np.pi\n",
    "                #samp_h_angles = samp_im[:,:,0] * 2.0 * np.pi\n",
    "                hsvImageCopy[:,:,0] = samp_h_angles\n",
    "                \n",
    "                #compute gradients\n",
    "                hsv_x_grad = np.zeros([num_y_samp-2, num_x_samp-2, 3])\n",
    "                hsv_y_grad = np.zeros([num_y_samp-2, num_x_samp-2, 3])\n",
    "                \n",
    "                hsv_x_grad[:,:,0] = np.min(\n",
    "                        [ np.abs(samp_im[1:-1,0:num_x_samp-2,0] - samp_im[1:-1,1:num_x_samp-1,0]),\n",
    "                        2.0 * np.pi - np.abs( samp_im[1:-1,0:num_x_samp-2,0] - samp_im[1:-1,1:num_x_samp-1,0])\n",
    "                        ],\n",
    "                        axis = 0\n",
    "                      )\n",
    "                hsv_y_grad[:,:,0] = np.min(\n",
    "                        [ np.abs(samp_im[0:num_y_samp-2,1:-1,0] - samp_im[1:num_y_samp-1,1:-1,0]),\n",
    "                        2.0 * np.pi - np.abs( samp_im[0:num_y_samp-2,1:-1,0] - samp_im[1:num_y_samp-1,1:-1,0])\n",
    "                        ],\n",
    "                        axis = 0\n",
    "                      )\n",
    "\n",
    "                hsv_x_grad[:,:,1:3] = np.abs(samp_im[1:-1,0:num_x_samp-2,1:3] - samp_im[1:-1,1:num_x_samp-1,1:3])\n",
    "                hsv_y_grad[:,:,1:3] = np.abs(samp_im[0:num_y_samp-2,1:-1,1:3] - samp_im[1:num_y_samp-1,1:-1,1:3])\n",
    "\n",
    "                hsv_grad_mag = np.sqrt(np.power(hsv_x_grad,2) + np.power(hsv_y_grad,2))\n",
    "                \n",
    "                h_pcts = np.percentile(hsv_grad_mag[:,:,0], [5,10,25,50,75,90,95])\n",
    "                s_pcts = np.percentile(hsv_grad_mag[:,:,1], [5,10,25,50,75,90,95])\n",
    "                v_pcts = np.percentile(hsv_grad_mag[:,:,2], [5,10,25,50,75,90,95])\n",
    "                \n",
    "                for ind, percentile in enumerate([5, 10, 25, 50, 75, 90, 95]):\n",
    "                    info.loc[i, 'h_cx_%.2i_pct' % percentile] = h_pcts[ind]\n",
    "                    info.loc[i, 's_cx_%.2i_pct' % percentile] = s_pcts[ind]\n",
    "                    info.loc[i, 'v_cx_%.2i_pct' % percentile] = v_pcts[ind]\n",
    "\n",
    "            #im = cv2.imread(dir+'/'+info.loc[i, 'new_filename'])\n",
    "            #info.loc[i, 'pixelsx'], info.loc[i, 'pixelsy'] = im.shape[0:2]\n",
    "            info.loc[i, 'size_bytes'] = os.path.getsize(mydir+'/'+info.loc[i, 'filename']) \n",
    "            if (ind+1)%100==0:\n",
    "                currentTime = time.clock()\n",
    "                print('Job %s, finished %s of %s, total time = %.2f min' %\n",
    "                     (jobNum, (ind+1), len(info), (currentTime - startTime)/60.0))\n",
    "        except:\n",
    "            print('job %s - error in %s' % (jobNum, mydir+'/'+info.loc[i, 'filename']))\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    currentTime = time.clock()\n",
    "    print('- Job %s, finished getting image info, total time = %.2f min' % ( jobNum, (currentTime - startTime) / 60.0))\n",
    "    \n",
    "    return info\n",
    "\n",
    "    #return info.rename(columns={'filename' : 'new_filename'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load image info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#load training info\n",
    "train_info = pd.read_csv(r'/data/training_data/train_info.csv', index_col=0)\n",
    "submission_info = pd.read_csv(r'/data/test_data/submission_info.csv', index_col=0)\n",
    "\n",
    "#shuffle and save info\n",
    "#train_info = train_info.iloc[np.random.permutation(len(train_info))]\n",
    "#submission_info = submission_info.iloc[np.random.permutation(len(submission_info))]\n",
    "\n",
    "#train_info.to_csv(r'/data/training_data/train_info.csv')\n",
    "#submission_info.to_csv(r'/data/test_data/submission_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create submission image info from submission pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submission image data is a bunch of images pairs, but we may want to work with a list of test images instead\n",
    "\n",
    "#submission_pairs = pd.read_csv(r'/data/test_data/submission_pairs.csv')\n",
    "#images = list(set(list(submission_info.image1.unique()) + list(submission_info.image2.unique())))\n",
    "#submission_info = pd.DataFrame(data=images, columns=['filename'])\n",
    "#submission_info = pd.read_csv(r'/data/test_data/submission_info.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pairs = pd.read_csv(r'/data/test_data/submission_pairs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching 5 jobs to make pairs\n",
      "finished 100 of 320 artists\n",
      "finished 100 of 316 artists\n",
      "finished 100 of 316 artists\n",
      "finished 100 of 316 artists\n",
      "finished 100 of 316 artists\n",
      "finished 200 of 320 artists\n",
      "finished 200 of 316 artists\n",
      "finished 200 of 316 artists\n",
      "finished 200 of 316 artists\n",
      "finished 200 of 316 artists\n",
      "finished 300 of 320 artists\n",
      "finished 300 of 316 artists\n",
      "make pairs completed\n",
      "finished 300 of 316 artists\n",
      "make pairs completed\n",
      "make pairs completed\n",
      "finished 300 of 316 artists\n",
      "make pairs completed\n",
      "finished 300 of 316 artists\n",
      "make pairs completed\n",
      "making pairs complete, time taken = 0.31 minutes\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "train_pairs = makePairsParent(train_info, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /data/training_data/train_pairs.csv \n"
     ]
    }
   ],
   "source": [
    "#save as csv\n",
    "filepath = r'/data/training_data/train_pairs.csv'\n",
    "print('saving to %s ' % filepath  )#    train_pairs.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load pairs\n",
    "train_pairs = pd.read_csv(r'/data/training_data/train_pairs.csv', index_col = 0)\n",
    "#submission_pairs = pd.read_csv(r'/data/test_data/submission_pairs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin computing features\n",
      "Launching 35 jobs\n",
      "Job 12, starting getting image info for images 684-740\n",
      "Job 19, starting getting image info for images 1083-1139\n",
      "Job 28, starting getting image info for images 1596-1652\n",
      "Job 9, starting getting image info for images 513-569\n",
      "Job 26, starting getting image info for images 1482-1538\n",
      "Job 2, starting getting image info for images 114-170\n",
      "Job 7, starting getting image info for images 399-455\n",
      "Job 34, starting getting image info for images 1938-1999\n",
      "Job 13, starting getting image info for images 741-797\n",
      "Job 21, starting getting image info for images 1197-1253\n",
      "Job 24, starting getting image info for images 1368-1424\n",
      "Job 32, starting getting image info for images 1824-1880\n",
      "Job 17, starting getting image info for images 969-1025\n",
      "Job 33, starting getting image info for images 1881-1937\n",
      "Job 30, starting getting image info for images 1710-1766\n",
      "Job 15, starting getting image info for images 855-911\n",
      "job 24 - error in /data/training_data/train/56414.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 20, starting getting image info for images 1140-1196\n",
      "Job 4, starting getting image info for images 228-284\n",
      "Job 6, starting getting image info for images 342-398\n",
      "Job 5, starting getting image info for images 285-341\n",
      "Job 25, starting getting image info for images 1425-1481\n",
      "Job 31, starting getting image info for images 1767-1823\n",
      "Job 29, starting getting image info for images 1653-1709\n",
      "Job 8, starting getting image info for images 456-512\n",
      "Job 3, starting getting image info for images 171-227\n",
      "Job 10, starting getting image info for images 570-626\n",
      "Job 0, starting getting image info for images 0-56\n",
      "Job 14, starting getting image info for images 798-854\n",
      "Job 22, starting getting image info for images 1254-1310\n",
      "Job 16, starting getting image info for images 912-968\n",
      "Job 18, starting getting image info for images 1026-1082\n",
      "Job 27, starting getting image info for images 1539-1595\n",
      "Job 1, starting getting image info for images 57-113\n",
      "Job 11, starting getting image info for images 627-683\n",
      "Job 23, starting getting image info for images 1311-1367\n",
      "job 25 - error in /data/training_data/train/38451.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 32 - error in /data/training_data/train/80711.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 27 - error in /data/training_data/train/48872.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72168.jpg is rgba\n",
      "job 9 - error in /data/training_data/train/92904.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 24 - error in /data/training_data/train/97185.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 3 - error in /data/training_data/train/77785.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 1 - error in /data/training_data/train/81985.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 19 - error in /data/training_data/train/71373.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 10 - error in /data/training_data/train/94058.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 2 - error in /data/training_data/train/100767.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 22 - error in /data/training_data/train/13224.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 29 - error in /data/training_data/train/64715.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Job 29, finished getting image info, total time = 0.12 min\n",
      "job 22 - error in /data/training_data/train/93049.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Job 24, finished getting image info, total time = 0.12 min\n",
      "- Job 33, finished getting image info, total time = 0.13 min\n",
      "- Job 23, finished getting image info, total time = 0.12 min\n",
      "- Job 26, finished getting image info, total time = 0.13 min\n",
      "job 19 - error in /data/training_data/train/102574.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Job 31, finished getting image info, total time = 0.13 min\n",
      "- Job 25, finished getting image info, total time = 0.13 min\n",
      "- Job 34, finished getting image info, total time = 0.14 min\n",
      "- Job 15, finished getting image info, total time = 0.13 min\n",
      "- Job 22, finished getting image info, total time = 0.13 min\n",
      "job 21 - error in /data/training_data/train/87321.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n",
      "IndexError: too many indices for array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Job 32, finished getting image info, total time = 0.14 min\n",
      "- Job 18, finished getting image info, total time = 0.13 min\n",
      "- Job 20, finished getting image info, total time = 0.13 min\n",
      "- Job 28, finished getting image info, total time = 0.15 min\n",
      "- Job 17, finished getting image info, total time = 0.14 min\n",
      "- Job 19, finished getting image info, total time = 0.14 min\n",
      "- Job 13, finished getting image info, total time = 0.14 min\n",
      "- Job 10, finished getting image info, total time = 0.13 min\n",
      "job 16 - error in /data/training_data/train/23571.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "IndexError: too many indices for array\n",
      "  File \"<ipython-input-151-9c9e49b38fe7>\", line 213, in getFeaturesWorker\n",
      "    samp_im[:,:,0] = samp_h_angles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Job 27, finished getting image info, total time = 0.14 min\n",
      "- Job 14, finished getting image info, total time = 0.13 min\n",
      "- Job 1, finished getting image info, total time = 0.12 min\n",
      "- Job 3, finished getting image info, total time = 0.13 min\n",
      "- Job 7, finished getting image info, total time = 0.13 min\n",
      "- Job 30, finished getting image info, total time = 0.15 min\n",
      "- Job 16, finished getting image info, total time = 0.14 min\n",
      "- Job 12, finished getting image info, total time = 0.15 min\n",
      "- Job 2, finished getting image info, total time = 0.14 min\n",
      "- Job 9, finished getting image info, total time = 0.15 min\n",
      "- Job 5, finished getting image info, total time = 0.14 min\n",
      "- Job 4, finished getting image info, total time = 0.14 min\n",
      "- Job 11, finished getting image info, total time = 0.15 min\n",
      "- Job 6, finished getting image info, total time = 0.14 min\n",
      "- Job 21, finished getting image info, total time = 0.16 min\n",
      "- Job 8, finished getting image info, total time = 0.15 min\n",
      "- Job 0, finished getting image info, total time = 0.14 min\n",
      "collecting features complete, time taken = 0.21 minutes\n",
      "Finished computing features, time taken = 0.21 min\n"
     ]
    }
   ],
   "source": [
    "print('Begin computing features')\n",
    "startTime = time.time()\n",
    "\n",
    "train_features = getFeaturesParent(False)\n",
    "\n",
    "endTime = time.time()\n",
    "print(\"Finished computing features, time taken = %.2f min\" % ((endTime-startTime)/60.0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "style           22\n",
       "genre           22\n",
       "date           505\n",
       "h_mean           1\n",
       "h_var            1\n",
       "s_mean           1\n",
       "s_std            1\n",
       "s_10_pct         1\n",
       "s_25_pct         1\n",
       "s_50_pct         1\n",
       "s_75_pct         1\n",
       "s_90_pct         1\n",
       "v_mean           1\n",
       "v_std            1\n",
       "v_10_pct         1\n",
       "v_25_pct         1\n",
       "v_50_pct         1\n",
       "v_75_pct         1\n",
       "v_90_pct         1\n",
       "h_cx_05_pct      1\n",
       "h_cx_10_pct      1\n",
       "h_cx_25_pct      1\n",
       "h_cx_50_pct      1\n",
       "h_cx_75_pct      1\n",
       "h_cx_90_pct      1\n",
       "h_cx_95_pct      1\n",
       "s_cx_05_pct      1\n",
       "s_cx_10_pct      1\n",
       "s_cx_25_pct      1\n",
       "s_cx_50_pct      1\n",
       "s_cx_75_pct      1\n",
       "s_cx_90_pct      1\n",
       "s_cx_95_pct      1\n",
       "v_cx_05_pct      1\n",
       "v_cx_10_pct      1\n",
       "v_cx_25_pct      1\n",
       "v_cx_50_pct      1\n",
       "v_cx_75_pct      1\n",
       "v_cx_90_pct      1\n",
       "v_cx_95_pct      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.isnull().sum()[train_features.isnull().sum()!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_features.to_csv(r'/data/test_data/test_features.csv')\n",
    "#test_features_sorted.to_csv(r'/data/test_data/test_features.csv')\n",
    "#y = pd.read_csv(r'/data/test_data/test_features.csv', index_col=0)\n",
    "#test_features_sorted = test_features_sorted.drop('Unnamed: 0',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_features.to_csv(r'/data/training_data/train_features_40000_end.csv')\n",
    "#test_features_sorted.to_csv(r'/data/test_data/test_features.csv')\n",
    "#y = pd.read_csv(r'/data/test_data/test_features.csv', index_col=0)\n",
    "#test_features_sorted = test_features_sorted.drop('Unnamed: 0',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load features\n",
    "train_features_0 = pd.read_csv(r'/data/training_data/train_features_0_20000.csv', index_col = 0)\n",
    "train_features_1 = pd.read_csv(r'/data/training_data/train_features_20000_40000.csv', index_col = 0)\n",
    "train_features_2 = pd.read_csv(r'/data/training_data/train_features_40000_end.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = pd.concat([train_features_0, train_features_1, train_features_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features = pd.read_csv(r'/data/test_data/test_features.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill nulls\n",
    "cols = test_features.columns.copy()\n",
    "\n",
    "cols.drop('is_grayscale')\n",
    "\n",
    "test_features = test_features[cols].fillna(test_features[cols].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional processing on feature - remove extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saved features are straight from the feature functions with no handling of nulls, etc\n",
    "# these have to be addressed prior to training/predicting\n",
    "\n",
    "#rgb_features = ['r_mean', 'r_med', 'r_std', 'g_mean', 'g_med',\n",
    "#                'g_std', 'b_mean', 'b_med', 'b_std',]\n",
    "\n",
    "#size_features =  [ 'pixelsx',\n",
    "#       'pixelsy', 'size_bytes' ]\n",
    "\n",
    "\n",
    "# take out unnecessary columns\n",
    "def removeColumns(features):\n",
    "    feature_names = ['pixelsx', 'pixelsy', 'size_bytes',\n",
    "                     'r_mean', 'r_med', 'r_std',\n",
    "                     'g_mean', 'g_med', 'g_std',\n",
    "                     'b_mean', 'b_med', 'b_std',\n",
    "                     'h_mean', 'h_var',\n",
    "                     's_mean', 's_med', 's_std',\n",
    "                     'v_mean', 'v_med', 'v_std',\n",
    "                     'is_grayscale']\n",
    "\n",
    "    features = features[ ['filename'] + feature_names ]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = removeColumns(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', 'pixelsx', 'pixelsy', 'size_bytes', 'r_mean', 'r_med',\n",
       "       'r_std', 'g_mean', 'g_med', 'g_std', 'b_mean', 'b_med', 'b_std',\n",
       "       'h_mean', 'h_var', 's_mean', 's_med', 's_std', 'v_mean', 'v_med',\n",
       "       'v_std', 'is_grayscale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature processing - add image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def addImageFeatures(features):\n",
    "    \"\"\"modifies in place\"\"\"\n",
    "    features['aspect_ratio'] = features['pixelsx']/features['pixelsy']\n",
    "    features['size_per_pixel'] = features['size_bytes']/features['pixelsx']/features['pixelsy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addImageFeatures(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addImageFeatures(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join features to pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#join pair data to image features\n",
    "\n",
    "def joinPairsToFeatures(pairs, features):\n",
    "    \n",
    "    feature_base_names = ['pixelsx', 'pixelsy', 'size_bytes', 'aspect_ratio', 'size_per_pixel',\n",
    "                     'r_mean', 'r_med', 'r_std',\n",
    "                     'g_mean', 'g_med', 'g_std',\n",
    "                     'b_mean', 'b_med', 'b_std',\n",
    "                     'h_mean', 'h_var',\n",
    "                     's_mean', 's_med', 's_std',\n",
    "                     'v_mean', 'v_med', 'v_std', \n",
    "                     'is_grayscale' ]\n",
    "\n",
    "    col_dict_1 = {}\n",
    "    col_dict_2 = {}\n",
    "\n",
    "    for feature in feature_base_names:\n",
    "        col_dict_1[feature] = '%s_1' % feature\n",
    "        col_dict_2[feature] = '%s_2' % feature\n",
    "\n",
    "    pairs = pairs.merge(features,\n",
    "                        left_on='image1', right_on='filename')\n",
    "    pairs.rename( columns = col_dict_1,\n",
    "                          inplace=True)\n",
    "    pairs = pairs.merge(features,\n",
    "                        left_on='image2', right_on='filename')\n",
    "    pairs.rename( columns = col_dict_2,\n",
    "                          inplace=True)\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pairs = joinPairsToFeatures(train_pairs, train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['artist1', 'image1', 'artist2', 'image2', 'sameArtist', 'foldNum',\n",
       "       'filename_x', 'pixelsx_1', 'pixelsy_1', 'size_bytes_1', 'r_mean_1',\n",
       "       'r_med_1', 'r_std_1', 'g_mean_1', 'g_med_1', 'g_std_1', 'b_mean_1',\n",
       "       'b_med_1', 'b_std_1', 'h_mean_1', 'h_var_1', 's_mean_1', 's_med_1',\n",
       "       's_std_1', 'v_mean_1', 'v_med_1', 'v_std_1', 'is_grayscale_1',\n",
       "       'aspect_ratio_1', 'size_per_pixel_1', 'filename_y', 'pixelsx_2',\n",
       "       'pixelsy_2', 'size_bytes_2', 'r_mean_2', 'r_med_2', 'r_std_2',\n",
       "       'g_mean_2', 'g_med_2', 'g_std_2', 'b_mean_2', 'b_med_2', 'b_std_2',\n",
       "       'h_mean_2', 'h_var_2', 's_mean_2', 's_med_2', 's_std_2', 'v_mean_2',\n",
       "       'v_med_2', 'v_std_2', 'is_grayscale_2', 'aspect_ratio_2',\n",
       "       'size_per_pixel_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pairs = joinPairsToFeatures(test_pairs, test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pairs get out of order after merging, so re-sort\n",
    "test_pairs = test_pairs.sort_values(by='index')\n",
    "test_pairs.set_index('index', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove nulls in training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nulls\n"
     ]
    }
   ],
   "source": [
    "# we remove the nulls after the join, could also be done before\n",
    "train_pairs = train_pairs[~train_pairs['pixelsx_1'].isnull()]\n",
    "train_pairs = train_pairs[~train_pairs['pixelsx_2'].isnull()]\n",
    "train_pairs = train_pairs[~train_pairs['size_bytes_1'].isnull()]\n",
    "train_pairs = train_pairs[~train_pairs['size_bytes_2'].isnull()]\n",
    "train_pairs = train_pairs[~train_pairs['h_mean_1'].isnull()]\n",
    "train_pairs = train_pairs[~train_pairs['h_mean_2'].isnull()]\n",
    "\n",
    "#print(train_pairs.isnull().sum())\n",
    "print('%i nulls' % (train_pairs.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check no nulls in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-922e2b8fb3a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(test_pairs.isnull().sum())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "#print(test_pairs.isnull().sum())\n",
    "print(test_pairs.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature processing - add diff features to pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## try training on diffs instead of aboslute values\n",
    "\n",
    "def addPairFeatures(pairs):\n",
    "    diff_feature_base = [ \n",
    "                     'r_mean', 'r_med', 'r_std',\n",
    "                     'g_mean', 'g_med', 'g_std',\n",
    "                     'b_mean', 'b_med', 'b_std',\n",
    "                     'h_mean', 'h_var',\n",
    "                     's_mean', 's_med', 's_std',\n",
    "                     'v_mean', 'v_med', 'v_std', ]\n",
    "\n",
    "    diff_feature_names = [ temp_feature + '_diff' for temp_feature in diff_feature_base]\n",
    "\n",
    "    for diff_feature in diff_feature_base:\n",
    "        pairs[diff_feature + '_diff'] = ( pairs[ diff_feature + '_1'] - pairs[ diff_feature + '_2'] ).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-a8baba5e131e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maddPairFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "addPairFeatures(test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "addPairFeatures(train_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computePredictStats(y_prob, y_true, threshold = 0.5):\n",
    "    \"\"\" compute accuracy, precision, recall, negative precision, specificity, and auc roc\n",
    "        Args:\n",
    "            y_prob: array of floats from 0.0 - 1.0\n",
    "            y_true: array of booleans\n",
    "            threshold: true/false threshold value, between 0.0-1.0\n",
    "        Returns:\n",
    "            dict of classification metrics\n",
    "    \"\"\"\n",
    "    # y_pred = np.array([True, True, False, False])\n",
    "    # y_true = np.array([True, True, True, True])\n",
    "    y_pred = y_prob > threshold\n",
    "    \n",
    "    total = len(y_prob)\n",
    "    true_pos = sum( (y_pred == True) & (y_true == True) )\n",
    "    true_neg = sum( (y_pred == False) & (y_true == False) )\n",
    "    false_pos = sum( (y_pred == True) & (y_true == False) )\n",
    "    false_neg = sum( (y_pred == False ) & (y_true == True) )\n",
    "    \n",
    "    accuracy = (true_pos + true_neg) / total\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    npp = true_neg / (true_neg + false_neg) #negative prediction value\n",
    "    specificity = true_neg / (true_neg + false_pos)\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    return { 'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'npp': npp,\n",
    "            'specificity': specificity,\n",
    "            'roc': roc,\n",
    "            'true_pos': true_pos,\n",
    "            'true_neg': true_neg,\n",
    "            'false_pos': false_pos,\n",
    "            'false_neg': false_neg,\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featureHelper(feature_name):\n",
    "    include = ( 'foldNum' in feature_name\n",
    "               or 'pixelsx' in feature_name\n",
    "               or 'pixelsy' in feature_name\n",
    "               #or 'aspect_ratio' in feature_name\n",
    "               #or '_diff' in feature_name\n",
    "               or 'size_bytes' in feature_name\n",
    "               or 'size_per_pixel' in feature_name\n",
    "               or 'h_mean_diff' in feature_name\n",
    "               or 'h_var_diff' in feature_name\n",
    "               or 's_mean_diff' in feature_name\n",
    "               or 's_med_diff' in feature_name\n",
    "               or 's_std_diff' in feature_name\n",
    "               or 'v_mean_diff' in feature_name\n",
    "               or 'v_med_diff' in feature_name\n",
    "               or 'v_std_diff' in feature_name\n",
    "               or 'r_' in feature_name\n",
    "               or 'g_' in feature_name\n",
    "               or 'b_' in feature_name\n",
    "              )\n",
    "    \n",
    "    if include:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for foldNum, train_pairs in enumerate(train_pairs_list):\n",
    "#    train_pairs_list[foldNum] = train_pairs.iloc[np.random.permutation(len(train_pairs))]\n",
    "\n",
    "# get list of X columns, they are ones with '_1' or '_2' in the name\n",
    "allCols = train_pairs.columns\n",
    "isX = list(map(featureHelper, allCols))\n",
    "X_columns = allCols[isX]\n",
    "\n",
    "train_X = train_pairs[X_columns]\n",
    "train_Y = train_pairs[['sameArtist', 'foldNum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-6330adc432f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mallCols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0misX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureHelper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallCols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mallCols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "allCols = test_pairs.columns\n",
    "isX = list(map(featureHelper, allCols))\n",
    "X_columns = allCols[isX]\n",
    "\n",
    "test_X = test_pairs[X_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['foldNum', 'pixelsx_1', 'pixelsy_1', 'size_bytes_1', 'r_mean_1',\n",
       "       'r_med_1', 'r_std_1', 'g_mean_1', 'g_med_1', 'g_std_1', 'b_mean_1',\n",
       "       'b_med_1', 'b_std_1', 'h_var_1', 'size_per_pixel_1', 'pixelsx_2',\n",
       "       'pixelsy_2', 'size_bytes_2', 'r_mean_2', 'r_med_2', 'r_std_2',\n",
       "       'g_mean_2', 'g_med_2', 'g_std_2', 'b_mean_2', 'b_med_2', 'b_std_2',\n",
       "       'h_var_2', 'size_per_pixel_2', 'r_mean_diff', 'r_med_diff',\n",
       "       'r_std_diff', 'g_mean_diff', 'g_med_diff', 'g_std_diff', 'b_mean_diff',\n",
       "       'b_med_diff', 'b_std_diff', 'h_mean_diff', 'h_var_diff', 's_mean_diff',\n",
       "       's_med_diff', 's_std_diff', 'v_mean_diff', 'v_med_diff', 'v_std_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-bfb20c524660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "test_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(test_X).sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kFoldCV(train_X, train_Y, k, numFoldsToTest):\n",
    "    \"\"\" Perform k-fold cross validation. Will modify train_pairs (shuffle rows)\n",
    "        Args:\n",
    "            train_pairs: Dataframe containing training image pairs with features. Should have no nulls.\n",
    "            k: k, at least 2\n",
    "            numFoldsToTest: how many folds to actually test, at most k\n",
    "        Returns:\n",
    "            dataframe with CV results\n",
    "    \"\"\"\n",
    "    numFoldsToTest = min(k, numFoldsToTest)\n",
    "    \n",
    "    # shuffle rows\n",
    "    #train_pairs = train_pairs.iloc[np.random.permutation(len(train_pairs))]\n",
    "\n",
    "    # get list of X columns, they are ones with '_1' or '_2' in the name\n",
    "    #allCols = train_pairs.columns\n",
    "    #isX = list(map(lambda x: ('_1' in x) or ('_2' in x), allCols))\n",
    "    #X_columns = allCols[isX]\n",
    "    \n",
    "    # split X and Y data\n",
    "    #CV_X = train_pairs[X_columns]\n",
    "    #CV_Y = train_pairs['sameArtist']\n",
    "    \n",
    "    # define which indices belong to each fold\n",
    "    foldLocsList = [] #list of Index objects, one for each fold\n",
    "    \n",
    "    #foldSize = int(len(train_X)/k)\n",
    "    #    \n",
    "    #for foldNum in range(k):\n",
    "    #    if foldNum == k-1:\n",
    "    #        foldLocsList.append( train_X.index[foldNum*foldSize : len(train_X)] )\n",
    "    #    else:\n",
    "    #        foldLocsList.append( train_X.index[foldNum*foldSize : (foldNum+1)*(foldSize) ] )\n",
    "    \n",
    "    # set up dataframe for collecting results\n",
    "    columnsList = list(itertools.product(('train', 'test'), ('roc', 'precision', 'recall', 'npp', 'specificity')))\n",
    "    results = pd.DataFrame(index = range(numFoldsToTest), columns = pd.MultiIndex.from_tuples(columnsList))\n",
    "    \n",
    "    # test each fold\n",
    "    for testFold in range(numFoldsToTest):\n",
    "        \n",
    "        CV_training_folds = np.arange(k)\n",
    "        CV_training_folds = np.delete(CV_training_folds, np.where(CV_training_folds == testFold))\n",
    "\n",
    "        # set up Xs\n",
    "        CV_train_X = train_X[ train_X['foldNum'].isin(CV_training_folds) ]   \n",
    "        CV_test_X = train_X[ train_X['foldNum'] == testFold ]   \n",
    "        \n",
    "        CV_train_X = CV_train_X.drop(['foldNum'], axis=1)\n",
    "        CV_test_X = CV_test_X.drop(['foldNum'], axis=1)\n",
    "\n",
    "        # set up Ys\n",
    "        CV_train_Y = train_Y[train_X['foldNum'].isin(CV_training_folds) ]\n",
    "        CV_test_Y = train_Y[ train_Y['foldNum'] == testFold ]   \n",
    "        \n",
    "        CV_train_Y = CV_train_Y['sameArtist']\n",
    "        CV_test_Y = CV_test_Y['sameArtist']\n",
    "\n",
    "        shuffleInds = np.random.permutation(len(CV_train_X))\n",
    "        \n",
    "        CV_train_X = CV_train_X.iloc[0:300000]\n",
    "        CV_train_Y = CV_train_Y.iloc[0:300000]\n",
    "        \n",
    "        # fit model\n",
    "        n_jobs = max( multiprocessing.cpu_count() - 2, 1)\n",
    "        #clf = RandomForestClassifier(n_estimators=500, n_jobs=n_jobs, min_samples_split=15, oob_score = True)\n",
    "        #clf = ExtraTreesClassifier(n_estimators=300, n_jobs=n_jobs, min_samples_split=5)\n",
    "        clf = GradientBoostingClassifier(n_estimators = 5)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        print('starting fit with %s jobs.. ' % n_jobs, end='')\n",
    "\n",
    "        clf.fit(CV_train_X, CV_train_Y)\n",
    "\n",
    "        end = time.time()\n",
    "                               \n",
    "        print('total training time: %s' % (end - start) )\n",
    "\n",
    "        # get in-sample and out-of-sample results\n",
    "                               \n",
    "        pred_train = clf.predict_proba(CV_train_X)[:,1]\n",
    "        train_results = computePredictStats( pred_train, CV_train_Y)\n",
    "\n",
    "        pred_test = clf.predict_proba(CV_test_X)[:,1]\n",
    "        test_results = computePredictStats( pred_test, CV_test_Y)\n",
    "       \n",
    "        for stat in ('roc', 'accuracy', 'precision', 'recall', 'npp', 'specificity'):\n",
    "            results.loc[testFold, ('train', stat)] = train_results[stat]\n",
    "            results.loc[testFold, ('test', stat)] = test_results[stat]\n",
    "        \n",
    "        #results.loc[testFold, ('train', 'oob')] = clf.oob_score_\n",
    "        \n",
    "        columnToImportance = list(zip(CV_train_X.columns, clf.feature_importances_))\n",
    "        columnToImportance = sorted(columnToImportance, key=lambda x: x[1], reverse=True)\n",
    "        importanceDF = pd.DataFrame(data = columnToImportance)\n",
    "            \n",
    "    return results, importanceDF    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run k-fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fit with 38 jobs.. total training time: 6.677391529083252\n",
      "starting fit with 38 jobs.. total training time: 6.667193174362183\n",
      "      train      test     train      test\n",
      "        roc       roc  accuracy  accuracy\n",
      "0  0.642529  0.609495  0.600230  0.578916\n",
      "1  0.626275  0.630901  0.590447  0.594956\n",
      "train  roc         0.634402\n",
      "test   roc         0.620198\n",
      "train  accuracy    0.595338\n",
      "test   accuracy    0.586936\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "numFoldsToTest = 2\n",
    "\n",
    "results, importance = kFoldCV(train_X, train_Y, k, numFoldsToTest)\n",
    "\n",
    "#print(results[ [('train', 'roc'), ('test', 'roc'), ('train', 'oob'),  ('test', 'accuracy')]])\n",
    "#print(results[ [('train', 'roc'), ('test', 'roc'), ('train', 'oob'),  ('test', 'accuracy')]].mean())\n",
    "print(results[ [('train', 'roc'), ('test', 'roc'), ('train', 'accuracy'), ('test', 'accuracy')]])\n",
    "print(results[ [('train', 'roc'), ('test', 'roc'), ('train', 'accuracy'), ('test', 'accuracy')]].mean())\n",
    "#print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      train      test    train      test\n",
      "        roc       roc accuracy  accuracy\n",
      "0  0.761061  0.667766  0.68926  0.620130\n",
      "1  0.723338  0.664235  0.65366  0.618091\n",
      "train  roc         0.742199\n",
      "test   roc         0.666001\n",
      "train  accuracy    0.671460\n",
      "test   accuracy    0.619111\n",
      "dtype: float64\n",
      "                   0         1\n",
      "0       size_bytes_2  0.053831\n",
      "1          pixelsx_2  0.051941\n",
      "2          pixelsx_1  0.051854\n",
      "3   size_per_pixel_2  0.051589\n",
      "4       size_bytes_1  0.042519\n",
      "5          pixelsy_2  0.040967\n",
      "6         b_med_diff  0.039091\n",
      "7          pixelsy_1  0.038525\n",
      "8        h_mean_diff  0.035571\n",
      "9         s_med_diff  0.035206\n",
      "10  size_per_pixel_1  0.033904\n",
      "11        v_med_diff  0.032000\n",
      "12        h_var_diff  0.030899\n",
      "13        v_std_diff  0.029114\n",
      "14           r_med_2  0.028642\n",
      "15          b_mean_2  0.028509\n",
      "16       v_mean_diff  0.026880\n",
      "17           r_std_1  0.026450\n",
      "18          b_mean_1  0.020881\n",
      "19           h_var_1  0.020002\n",
      "20          r_mean_2  0.019903\n",
      "21        r_std_diff  0.018074\n",
      "22           h_var_2  0.017972\n",
      "23        g_med_diff  0.017683\n",
      "24           b_med_1  0.017553\n",
      "25           b_std_2  0.017032\n",
      "26          g_mean_1  0.016238\n",
      "27           r_std_2  0.015816\n",
      "28           r_med_1  0.015686\n",
      "29           g_med_2  0.014638\n",
      "30           b_std_1  0.014040\n",
      "31          g_mean_2  0.011498\n",
      "32        s_std_diff  0.011446\n",
      "33           g_med_1  0.010852\n",
      "34        b_std_diff  0.009530\n",
      "35           b_med_2  0.008585\n",
      "36       b_mean_diff  0.007249\n",
      "37           g_std_1  0.005953\n",
      "38       s_mean_diff  0.005838\n",
      "39        r_med_diff  0.005660\n",
      "40          r_mean_1  0.005325\n",
      "41        g_std_diff  0.005268\n",
      "42           g_std_2  0.004470\n",
      "43       g_mean_diff  0.003485\n",
      "44       r_mean_diff  0.001833\n"
     ]
    }
   ],
   "source": [
    "print(results[ [('train', 'roc'), ('test', 'roc'), ('train', 'accuracy'), ('test', 'accuracy')]])\n",
    "print(results[ [('train', 'roc'), ('test', 'roc'), ('train', 'accuracy'), ('test', 'accuracy')]].mean())\n",
    "print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_Y_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997446"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fit\n",
      "total training time: 37.411495208740234\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, min_samples_split=15, n_jobs=30)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "print('starting fit')\n",
    "#excluding the patient_id column from the fit and prediction\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('total training time: %s' % (end - start) )\n",
    "\n",
    "#columnsList = list(itertools.product(('train', 'test'), ('roc', 'precision', 'recall', 'npp', 'specificity')))\n",
    "#results = pd.DataFrame(index = range(numFoldsToTest), columns = pd.MultiIndex.from_tuples(columnsList))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total saving time: 3.6660020000003897\n"
     ]
    }
   ],
   "source": [
    "##save model\n",
    "\n",
    "start = time.clock()\n",
    "\n",
    "with open('my_dumped_classifier.pkl', 'wb') as fid:\n",
    "    pickle.dump(clf, fid) \n",
    "\n",
    "end = time.clock()\n",
    "print('total saving time: %s' % (end - start) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total loading time: 0.007420999999339983\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "\n",
    "# load it again\n",
    "with open('my_dumped_classifier.pkl', 'rb') as fid:\n",
    "    clf = pickle.load(fid)\n",
    "\n",
    "end = time.clock()\n",
    "print('total loading time: %s' % (end - start) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predictions = clf.predict_proba(test_X)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## prepare submission\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(index = test_pairs.index)\n",
    "submission['sameArtist'] = test_predictions\n",
    "submission.to_csv('/data/notebook/notebooks/my_submission_01.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(index = test_pairs.index)\n",
    "submission['sameArtist'] = test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sameArtist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.146189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.915134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.395387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.117785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sameArtist\n",
       "index            \n",
       "0        0.146189\n",
       "1        0.915134\n",
       "2        0.062293\n",
       "3        0.395387\n",
       "4        0.117785"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21916047"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
